{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5360db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ca5c2",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e058f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Data preprocess: \n",
    "# 1. creat train_sentence,train_sentence_label,vocabulary.\n",
    "# 2. add '<unk>','<padding>' in vocabulary\n",
    "# 3. padding each sentence\n",
    "# 4. creat train_sentence_indices\n",
    "\n",
    "ner_categories = ['person', 'geo-loc', 'company', 'facility', 'product',\n",
    "            'musicartist', 'movie', 'sportsteam', 'tvshow', 'other']\n",
    "ner_format = ['B','I','O']\n",
    "\n",
    "def convert_to_indices(sentence,to_idx):\n",
    "    # input example: ['I','am','a','pig']\n",
    "    indices = []\n",
    "    for token in sentence:\n",
    "        if token in to_idx: indices.append(to_idx[token])\n",
    "        else: indices.append(to_idx['<unk>'])\n",
    "    return indices\n",
    "\n",
    "def get_data(path,ner_categories,ner_format,train_vocabulary=None):\n",
    "    vocabulary = set()\n",
    "    train_sentence = []\n",
    "    train_sentence_label = []\n",
    "    with open(path,'r') as f1:\n",
    "        lines = f1.readlines()\n",
    "        sentence = []\n",
    "        label = []\n",
    "        for line in lines:\n",
    "            if line!='\\n':\n",
    "                TAB_idx = line.index('\\t')\n",
    "                word = line[:TAB_idx]\n",
    "                # 全部轉小寫\n",
    "                word = word.lower()\n",
    "                if re.match('http:',word):\n",
    "                    vocabulary.add('<url>')\n",
    "                    sentence.append('<url>')\n",
    "                elif word[0]=='@' and len(word)>1:\n",
    "                    vocabulary.add('<user>') \n",
    "                    sentence.append('<user>')\n",
    "                else:\n",
    "                    vocabulary.add(word)\n",
    "                    sentence.append(word)\n",
    "                label.append(line[TAB_idx+1:-1])\n",
    "            else:\n",
    "                train_sentence.append(sentence)\n",
    "                train_sentence_label.append(label)\n",
    "                sentence = []\n",
    "                label = []\n",
    "    vocabulary.add('<unk>')\n",
    "    vocabulary.add('<padding>')\n",
    "    #找出最長的句子，其他不夠長的要padding\n",
    "    #max_sentence_len = len(max(train_sentence, key=lambda x: len(x)))\n",
    "    max_sentence_len = 41\n",
    "    \n",
    "    # process padding\n",
    "    for idx,sentence in enumerate(train_sentence):\n",
    "        if len(sentence)<max_sentence_len:\n",
    "            train_sentence[idx] = sentence+['<padding>']*(max_sentence_len-len(sentence))\n",
    "    for idx,label in enumerate(train_sentence_label):\n",
    "        if len(label)<max_sentence_len:\n",
    "            train_sentence_label[idx] = label+['O']*(max_sentence_len-len(label))\n",
    "            \n",
    "    # get idx_to_word,word_to_idx\n",
    "    if train_vocabulary!=None: vocabulary=train_vocabulary\n",
    "    idx_to_word = sorted(list(vocabulary))\n",
    "    word_to_idx = {word: ind for ind, word in enumerate(idx_to_word)}\n",
    "    \n",
    "    #\n",
    "    train_vector = []\n",
    "    for sentence in train_sentence:\n",
    "        indices = convert_to_indices(sentence,word_to_idx)\n",
    "        train_vector.append(indices)\n",
    "    #\n",
    "    idx_to_label = []\n",
    "    for category in ner_categories:\n",
    "        for j in ner_format[:2]:\n",
    "            idx_to_label.append(j+'-'+category)\n",
    "    idx_to_label.append('O')\n",
    "    label_to_idx = {label:i for i,label in enumerate(idx_to_label)}\n",
    "    \n",
    "    # get train_vector_label\n",
    "    train_vector_label = []\n",
    "    for label in train_sentence_label:\n",
    "        indices = convert_to_indices(label,label_to_idx)\n",
    "        train_vector_label.append(indices)\n",
    "    \n",
    "    return train_vector,train_vector_label,vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    '''Dataset for loading and preprocessing'''\n",
    "    def __init__(self, mode, x_vectors, y_vectors=None):\n",
    "        self.mode = mode\n",
    "        self.x_vectors = x_vectors\n",
    "        self.y_vectors = y_vectors\n",
    "    def __len__(self):\n",
    "        return len(self.x_vectors)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_vectors[idx]\n",
    "        if self.mode=='train' or self.mode=='val':\n",
    "            y = self.y_vectors[idx]\n",
    "            return torch.tensor(x),torch.tensor(y)\n",
    "        else:\n",
    "            return torch.tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "# data\n",
    "batch_size = 128\n",
    "# model\n",
    "NER_class = 21\n",
    "max_sentence_len = 41\n",
    "lstm_input_size = max_sentence_len\n",
    "embedding_dim = 100\n",
    "lstm_hidden_size = 128\n",
    "lstm_num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8620422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.train_sentence: [['I','am','a','pig'],['HAHA'],['Yes','you','are']]\n",
    "# 1.train_vector: [[0,1,2,5],[29],[33,8,100]] 後面加padding\n",
    "\n",
    "# 5.train_sentence_label: [['B-person','O','O','B-other'],['B-other'],['O','B-person','O']]\n",
    "# 2.train_vector_label: [[0,20,20,18],[18],[20,18,20]] 後面加padding\n",
    "\n",
    "# 3.vocabulary: set('I','am','a',......,'are')\n",
    "\n",
    "# 6.idx_to_word: sorted(list(vocabulary))\n",
    "# 7.word_to_idx: {'I':0,'am':1, 'a':2,......,'are':7}\n",
    "# 8.idx_to_label: ['B-person', 'I-person',......,'O']\n",
    "# 9.label_to_idx: {'B-person':0, 'I-person':1,......,'O':20}\n",
    "\n",
    "\n",
    "# Get train/validation data\n",
    "train_vector,train_vector_label, train_vocabulary = get_data('./train.txt',ner_categories,ner_format)\n",
    "val_vector,val_vector_label, _ = get_data('./dev.txt',ner_categories,ner_format,train_vocabulary)\n",
    "\n",
    "train_data = CustomDataset(mode='train', x_vectors=train_vector, y_vectors=train_vector_label)\n",
    "train_loader = data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "val_data = CustomDataset(mode='val', x_vectors=val_vector, y_vectors=val_vector_label)\n",
    "val_loader = data.DataLoader(dataset=val_data, batch_size=1, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d2c5b",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451813c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat model\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class NER_model(nn.Module):\n",
    "    def __init__(self, NER_class, lstm_input_size, lstm_hidden_size, lstm_num_layers):\n",
    "        super(NER_model, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=len(train_vocabulary), embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_hidden_size,\n",
    "                            num_layers=lstm_num_layers, batch_first =True, bidirectional=True,\n",
    "                            dropout=0.5)\n",
    "        self.fc = nn.Linear(lstm_hidden_size*2, NER_class)\n",
    "    def forward(self,x):\n",
    "        embed = self.embed(x)\n",
    "        lstm_out, (h_n,c_n) = self.lstm(embed)\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        return fc_out\n",
    "model = NER_model(NER_class, lstm_input_size, lstm_hidden_size, lstm_num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281086ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# caculate f1 score\n",
    "def cal_metrics(model,val_loader,device):\n",
    "    model.eval()\n",
    "    y_label = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for x,y in val_loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        y_label = y_label+y[0].cpu().numpy().tolist()\n",
    "        with torch.no_grad():\n",
    "            model_pred = model(x)\n",
    "            flatten_pred = torch.argmax(model_pred, dim=2).cpu().numpy().tolist()\n",
    "            y_pred = y_pred+flatten_pred[0]\n",
    "    return f1_score(y_label,y_pred,average='macro')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# training\n",
    "epochs = 1000\n",
    "lr = 0.01\n",
    "loss_fun = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "min_val_loss = 10000\n",
    "max_f1score = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    print('epoch {}:'.format(epoch))\n",
    "    metrics = {'accuracy':None,'precision':None,'recall':None,'f1':None}\n",
    "    \n",
    "    temp = []\n",
    "    for x,y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        weight_loss = torch.tensor([0]).to(device).double()\n",
    "        x,y = x.to(device), y.to(device)    # x.size(): batch_size*seqlen*n_class, y.size(): batch_size*seqlen\n",
    "        model_pred = model(x)    # model_pred.shape: batch*seq_len*n_class\n",
    "        # 讓 O label的loss比較小\n",
    "        loss = loss_fun(model_pred.permute(0, 2, 1), y)\n",
    "        for i in range(len(y)):\n",
    "            for j in range(41):\n",
    "                if y[i][j]==20:weight_loss+=(0.5*loss[i][j])\n",
    "                else:weight_loss+=loss[i][j]\n",
    "        weight_loss/=(256*41)\n",
    "        temp.append(weight_loss)\n",
    "        weight_loss.backward()\n",
    "        optimizer.step()\n",
    "    print('train loss:{}'.format((sum(temp)/len(train_loader)).item()))\n",
    "    ## calculate metrics\n",
    "    '''\n",
    "    metrics['accuracy'] = accuracy_score(y_label,y_pred)\n",
    "    metrics['precision'] = precision_score(y_label,y_pred,average=None)\n",
    "    metrics['recall'] = recall_score(y_label,y_pred,average=None)\n",
    "    '''\n",
    "    #metrics['f1'] = f1_score(y_label,y_pred,average='macro')\n",
    "    #print(np.round(f1_score(y_label,y_pred,average=None),3))\n",
    "    # 依照 f1 score save model\n",
    "    metrics['f1']  = cal_metrics(model,val_loader,device)\n",
    "    if metrics['f1']>max_f1score:\n",
    "        max_f1score=metrics['f1']\n",
    "        torch.save({'best_state_dict':model.state_dict()},'/home/mick/NLP_HW/best_f1score.pth')\n",
    "        print('save new high f1 score:{}\\n'.format(metrics['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validation result\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "checkpoint  = torch.load('./best_f1score.pth')\n",
    "model.load_state_dict(checkpoint['best_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "#val_data = CustomDataset(mode='val', x_vectors=val_vector, y_vectors=val_vector_label)\n",
    "#val_loader = data.DataLoader(dataset=val_data, batch_size=1, shuffle=False, drop_last=False)\n",
    "\n",
    "y_label = []\n",
    "y_pred = []\n",
    "for x,y in val_loader:\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    # x.size(): batch_size*seqlen*n_class\n",
    "    # y.size(): batch_size*seqlen\n",
    "    y_label = y_label+y[0].cpu().numpy().tolist()\n",
    "    with torch.no_grad():\n",
    "        model_pred = model(x)\n",
    "        flatten_pred = torch.argmax(model_pred, dim=2).cpu().numpy().tolist()\n",
    "        y_pred = y_pred+flatten_pred[0]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_label,y_pred)\n",
    "precision = precision_score(y_label,y_pred,average=None)\n",
    "recall = recall_score(y_label,y_pred,average=None)\n",
    "f1 = f1_score(y_label,y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89065075",
   "metadata": {},
   "source": [
    "# generate result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be298d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!two step\n",
    "def get_test_data(path,ner_categories,ner_format,train_vocabulary=None):\n",
    "    vocabulary = set()\n",
    "    train_sentence = []\n",
    "    with open(path,'r') as f1:\n",
    "        lines = f1.readlines()\n",
    "        sentence = []\n",
    "        for line in lines:\n",
    "            if line!='\\n':\n",
    "                word = line[:-1]\n",
    "                # 全部轉小寫\n",
    "                #word = word.lower()\n",
    "                if re.match('http:',word):\n",
    "                    vocabulary.add(word)\n",
    "                    sentence.append(word)\n",
    "                    #vocabulary.add('<url>')\n",
    "                    #sentence.append('<url>')\n",
    "                elif word[0]=='@' and len(word)>1:\n",
    "                    vocabulary.add(word)\n",
    "                    sentence.append(word)\n",
    "                    #vocabulary.add('<user>') \n",
    "                    #sentence.append('<user>')\n",
    "                else:\n",
    "                    vocabulary.add(word)\n",
    "                    sentence.append(word)\n",
    "            else:\n",
    "                train_sentence.append(sentence)\n",
    "                sentence = []\n",
    "    vocabulary.add('<unk>')\n",
    "    vocabulary.add('<padding>')\n",
    "    #找出最長的句子，其他不夠長的要padding\n",
    "    #max_sentence_len = len(max(train_sentence, key=lambda x: len(x)))\n",
    "    max_sentence_len = 41\n",
    "    for i in train_sentence:\n",
    "        if len(i)>41:print('True')\n",
    "    \n",
    "    # process padding\n",
    "    for idx,sentence in enumerate(train_sentence):\n",
    "        if len(sentence)<max_sentence_len:\n",
    "            train_sentence[idx] = sentence+['<padding>']*(max_sentence_len-len(sentence))\n",
    "            \n",
    "    # get idx_to_word,word_to_idx\n",
    "    if train_vocabulary!=None: vocabulary=train_vocabulary\n",
    "    idx_to_word = sorted(list(vocabulary))\n",
    "    word_to_idx = {word: ind for ind, word in enumerate(idx_to_word)}\n",
    "    \n",
    "    #\n",
    "    train_vector = []\n",
    "    for sentence in train_sentence:\n",
    "        indices = convert_to_indices(sentence,word_to_idx)\n",
    "        train_vector.append(indices)\n",
    "        \n",
    "    #\n",
    "    idx_to_label = []\n",
    "    for category in ner_categories:\n",
    "        for j in ner_format[:2]:\n",
    "            idx_to_label.append(j+'-'+category)\n",
    "    idx_to_label.append('O')\n",
    "    label_to_idx = {label:i for i,label in enumerate(idx_to_label)}\n",
    "\n",
    "    return train_vector,word_to_idx,idx_to_label,train_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector,word_to_idx,idx_to_label,train_sentence = get_test_data('./result.txt',ner_categories,ner_format,train_vocabulary)\n",
    "\n",
    "test_data = CustomDataset(mode='test', x_vectors=test_vector)\n",
    "test_loader = data.DataLoader(dataset=test_data, batch_size=1, shuffle=False, drop_last=False)\n",
    "\n",
    "_,_,_,train_sentence = get_test_data('./result.txt',ner_categories,ner_format,train_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b9950",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./best_f1score.pth')\n",
    "model.load_state_dict(checkpoint['best_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "#val_data = CustomDataset(mode='val', x_vectors=val_vector, y_vectors=val_vector_label)\n",
    "#val_loader = data.DataLoader(dataset=val_data, batch_size=1, shuffle=False, drop_last=False)\n",
    "\n",
    "y_pred = []\n",
    "for x in test_loader:\n",
    "    x = x.to(device)\n",
    "    # x.size(): batch_size*seqlen*n_class\n",
    "    # y.size(): batch_size*seqlen\n",
    "    with torch.no_grad():\n",
    "        model_pred = model(x)\n",
    "        flatten_pred = torch.argmax(model_pred, dim=2).cpu().numpy().tolist()\n",
    "        y_pred = y_pred+flatten_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output.txt','w') as f1,open('./result.txt','r') as f2:\n",
    "    lines2 = f2.readlines()\n",
    "    lines = []\n",
    "    count = -1\n",
    "    for sentence in train_sentence:\n",
    "        for token in sentence:\n",
    "            count+=1\n",
    "            if token!= '<padding>':\n",
    "                lines.append(token+'\\t'+idx_to_label[y_pred[count]]+'\\n')\n",
    "        lines.append('\\n')\n",
    "    f1.writelines(lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
